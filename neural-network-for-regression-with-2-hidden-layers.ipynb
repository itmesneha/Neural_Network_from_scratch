{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:15.682263Z","iopub.execute_input":"2025-10-11T08:39:15.682541Z","iopub.status.idle":"2025-10-11T08:39:15.686638Z","shell.execute_reply.started":"2025-10-11T08:39:15.682521Z","shell.execute_reply":"2025-10-11T08:39:15.685626Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"N = 64 # batch size\nD_in = 1000 # input dimension (# features)\nH = 100 # hidden layer dimension (# nodes)\nD_out = 10 # output classes\n\nx, y = np.random.randn(N, D_in), np.random.randn(N, D_out) # random input & output values\n\n# x = 64 x 1000\n# y = 64 x 10\n\n# W1 = 1000 x 100\n# W2 = 100 x 10\n# op = 10 x 1\n\nw1, w2 = np.random.randn(D_in, H), np.random.randn(H, D_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:17.377329Z","iopub.execute_input":"2025-10-11T08:39:17.377620Z","iopub.status.idle":"2025-10-11T08:39:17.388069Z","shell.execute_reply.started":"2025-10-11T08:39:17.377597Z","shell.execute_reply":"2025-10-11T08:39:17.387266Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# activation with first layer\nh = 1 / (1 + np.exp(-x.dot(w1)))\nh.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:13.467983Z","iopub.status.idle":"2025-10-11T08:39:13.468317Z","shell.execute_reply.started":"2025-10-11T08:39:13.468148Z","shell.execute_reply":"2025-10-11T08:39:13.468163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = h.dot(w2)\ny_pred.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:13.470175Z","iopub.status.idle":"2025-10-11T08:39:13.470453Z","shell.execute_reply.started":"2025-10-11T08:39:13.470322Z","shell.execute_reply":"2025-10-11T08:39:13.470338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# back propagation loss formula\nloss = np.square(y_pred - y).sum()\nloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:13.471893Z","iopub.status.idle":"2025-10-11T08:39:13.472189Z","shell.execute_reply.started":"2025-10-11T08:39:13.472051Z","shell.execute_reply":"2025-10-11T08:39:13.472064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grad_y_pred = 2.0 * (y_pred - y)\ngrad_y_pred.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:13.472903Z","iopub.status.idle":"2025-10-11T08:39:13.473169Z","shell.execute_reply.started":"2025-10-11T08:39:13.473054Z","shell.execute_reply":"2025-10-11T08:39:13.473066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grad_w2 = h.T.dot(grad_y_pred) # (100, 64) * (64, 10)\ngrad_w2.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:13.473943Z","iopub.status.idle":"2025-10-11T08:39:13.474350Z","shell.execute_reply.started":"2025-10-11T08:39:13.474191Z","shell.execute_reply":"2025-10-11T08:39:13.474207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grad_h = grad_y_pred.dot(w2.T)\ngrad_h.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:13.475643Z","iopub.status.idle":"2025-10-11T08:39:13.475971Z","shell.execute_reply.started":"2025-10-11T08:39:13.475778Z","shell.execute_reply":"2025-10-11T08:39:13.475794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grad_w1 = (x.T).dot(grad_h * h * (1 - h))\n# (1000, 64) * (64,100) = (1000, 100)\n# (100, 64) * (64, 100)\ngrad_w1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:13.476643Z","iopub.status.idle":"2025-10-11T08:39:13.476858Z","shell.execute_reply.started":"2025-10-11T08:39:13.476752Z","shell.execute_reply":"2025-10-11T08:39:13.476762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# putting it all together\nfor iteration in range(5000):\n    \n    # forward propagation\n\n    # sigmoid activation\n    h = 1 / (1 + np.exp(-x.dot(w1))) # (64, 100) \n    y_pred = h.dot(w2) # (64, 10)\n    loss = np.square(y_pred - y).sum()\n    if iteration % 20 == 0:\n        print('iteration: ', iteration, 'loss: ', loss)\n\n    # calculate gradients for back propagation\n    grad_y_pred = 2.0 * (y_pred - y) # (64 , 10)\n    grad_w2 = h.T.dot(grad_y_pred) # (100, 64) * (64, 10) = (100, 10)\n    grad_h = grad_y_pred.dot(w2.T) # (64, 10) * (10 * 100) = (64, 100)\n    grad_w1 = (x.T).dot(grad_h * h * (1 - h)) # (1000, 64) * (64,100) * (100, 64) * (64, 100) = (1000, 100)\n\n    LR = 1e-4\n    w1 -= LR * grad_w1\n    w2 -= LR * grad_w2\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T08:39:26.871784Z","iopub.execute_input":"2025-10-11T08:39:26.872582Z","iopub.status.idle":"2025-10-11T08:39:31.211950Z","shell.execute_reply.started":"2025-10-11T08:39:26.872551Z","shell.execute_reply":"2025-10-11T08:39:31.211151Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"iteration:  0 loss:  32536.390312386357\niteration:  20 loss:  6387.027209186369\niteration:  40 loss:  4554.042679801955\niteration:  60 loss:  3411.718669346021\niteration:  80 loss:  2668.0802819952005\niteration:  100 loss:  2129.3333675237604\niteration:  120 loss:  1747.1135979169126\niteration:  140 loss:  1471.5557584138319\niteration:  160 loss:  1239.6513629466372\niteration:  180 loss:  1065.0929166319816\niteration:  200 loss:  928.4114369686455\niteration:  220 loss:  819.6173356591263\niteration:  240 loss:  719.6786463501097\niteration:  260 loss:  642.5128103027009\niteration:  280 loss:  577.6835443353361\niteration:  300 loss:  521.2944275877046\niteration:  320 loss:  469.6195889928462\niteration:  340 loss:  425.4385104642733\niteration:  360 loss:  388.2129083307922\niteration:  380 loss:  354.6560037932403\niteration:  400 loss:  324.8662814188174\niteration:  420 loss:  297.925091885833\niteration:  440 loss:  271.9183306664873\niteration:  460 loss:  250.26493388026915\niteration:  480 loss:  230.96601184811908\niteration:  500 loss:  213.38304188715568\niteration:  520 loss:  197.0785371804092\niteration:  540 loss:  182.3606713118548\niteration:  560 loss:  169.2381566387195\niteration:  580 loss:  157.27708572464024\niteration:  600 loss:  146.2020610018412\niteration:  620 loss:  135.92308360625447\niteration:  640 loss:  126.52213852931061\niteration:  660 loss:  117.92540991872842\niteration:  680 loss:  110.1726045175929\niteration:  700 loss:  103.22944566249117\niteration:  720 loss:  96.89841846726029\niteration:  740 loss:  91.08492108837655\niteration:  760 loss:  85.72858891157493\niteration:  780 loss:  80.77880539620514\niteration:  800 loss:  76.19163028325916\niteration:  820 loss:  71.92896555011563\niteration:  840 loss:  67.95818256013925\niteration:  860 loss:  64.25177040745538\niteration:  880 loss:  60.78654707371564\niteration:  900 loss:  57.54250966191755\niteration:  920 loss:  54.50201245258209\niteration:  940 loss:  51.64997346952826\niteration:  960 loss:  48.97447432806095\niteration:  980 loss:  46.465313693214654\niteration:  1000 loss:  44.11086065318475\niteration:  1020 loss:  41.89760422124684\niteration:  1040 loss:  39.81244259424832\niteration:  1060 loss:  37.84388925303063\niteration:  1080 loss:  35.98160900937498\niteration:  1100 loss:  34.215788751294625\niteration:  1120 loss:  32.53759045199804\niteration:  1140 loss:  30.941310352579357\niteration:  1160 loss:  29.42745192891716\niteration:  1180 loss:  28.002021029069788\niteration:  1200 loss:  26.66773379088488\niteration:  1220 loss:  25.418501483090502\niteration:  1240 loss:  24.244847536581723\niteration:  1260 loss:  23.138494205907342\niteration:  1280 loss:  22.09267880438435\niteration:  1300 loss:  21.10151934699121\niteration:  1320 loss:  20.15962908933252\niteration:  1340 loss:  19.262070755407784\niteration:  1360 loss:  18.40480800135176\niteration:  1380 loss:  17.58588942490387\niteration:  1400 loss:  16.806643243566953\niteration:  1420 loss:  16.070133327352472\niteration:  1440 loss:  15.376646735840966\niteration:  1460 loss:  14.722517258156316\niteration:  1480 loss:  14.103234310583634\niteration:  1500 loss:  13.515359319523903\niteration:  1520 loss:  12.95643722805178\niteration:  1540 loss:  12.42454583192856\niteration:  1560 loss:  11.918040706813219\niteration:  1580 loss:  11.435444918089066\niteration:  1600 loss:  10.975399509053593\niteration:  1620 loss:  10.536638653306783\niteration:  1640 loss:  10.117976652004508\niteration:  1660 loss:  9.71830129282697\niteration:  1680 loss:  9.336570454968124\niteration:  1700 loss:  8.971810018571093\niteration:  1720 loss:  8.623111898886734\niteration:  1740 loss:  8.289631514995147\niteration:  1760 loss:  7.970584306364475\niteration:  1780 loss:  7.665241095782486\niteration:  1800 loss:  7.372922235687978\niteration:  1820 loss:  7.0929906690310185\niteration:  1840 loss:  6.824844369841651\niteration:  1860 loss:  6.56790906686743\niteration:  1880 loss:  6.321632493465453\niteration:  1900 loss:  6.085481379618965\niteration:  1920 loss:  5.858941887142175\niteration:  1940 loss:  5.641523360517922\niteration:  1960 loss:  5.432764486382492\niteration:  1980 loss:  5.232240477620192\niteration:  2000 loss:  5.039569669372584\niteration:  2020 loss:  4.854417707334923\niteration:  2040 loss:  4.676497299279074\niteration:  2060 loss:  4.505561737105916\niteration:  2080 loss:  4.341391790520852\niteration:  2100 loss:  4.183778284315357\niteration:  2120 loss:  4.032505449936536\niteration:  2140 loss:  3.8873407197588805\niteration:  2160 loss:  3.7480338357095997\niteration:  2180 loss:  3.614323644146805\niteration:  2200 loss:  3.48594798801341\niteration:  2220 loss:  3.3626523259294343\niteration:  2240 loss:  3.244194976773006\niteration:  2260 loss:  3.130349094695497\niteration:  2280 loss:  3.020902514283238\niteration:  2300 loss:  2.9156566173317975\niteration:  2320 loss:  2.814424963363872\niteration:  2340 loss:  2.717032026612988\niteration:  2360 loss:  2.6233121367901973\niteration:  2380 loss:  2.5331086117196824\niteration:  2400 loss:  2.44627303871636\niteration:  2420 loss:  2.362664663243825\niteration:  2440 loss:  2.282149854091832\niteration:  2460 loss:  2.2046016246417874\niteration:  2480 loss:  2.1298991973419845\niteration:  2500 loss:  2.0579276034546994\niteration:  2520 loss:  1.9885773132095808\niteration:  2540 loss:  1.9217438933659117\niteration:  2560 loss:  1.857327690295976\niteration:  2580 loss:  1.7952335373252521\niteration:  2600 loss:  1.735370485361602\niteration:  2620 loss:  1.677651555915547\niteration:  2640 loss:  1.6219935155266536\niteration:  2660 loss:  1.5683166704266422\niteration:  2680 loss:  1.5165446800399098\niteration:  2700 loss:  1.466604387694853\niteration:  2720 loss:  1.4184256667351676\niteration:  2740 loss:  1.3719412801081596\niteration:  2760 loss:  1.3270867514845004\niteration:  2780 loss:  1.2838002460324835\niteration:  2800 loss:  1.2420224591212081\niteration:  2820 loss:  1.2016965114415847\niteration:  2840 loss:  1.1627678492882454\niteration:  2860 loss:  1.1251841490140722\niteration:  2880 loss:  1.0888952249296087\niteration:  2900 loss:  1.053852940155239\niteration:  2920 loss:  1.020011120132301\niteration:  2940 loss:  0.9873254686558777\niteration:  2960 loss:  0.9557534864055495\niteration:  2980 loss:  0.9252543920256119\niteration:  3000 loss:  0.8957890458473222\niteration:  3020 loss:  0.8673198763611185\niteration:  3040 loss:  0.8398108095419692\niteration:  3060 loss:  0.8132272011127938\niteration:  3080 loss:  0.7875357718045617\niteration:  3100 loss:  0.7627045456405722\niteration:  3120 loss:  0.7387027912407811\niteration:  3140 loss:  0.7155009661108076\niteration:  3160 loss:  0.693070663851965\niteration:  3180 loss:  0.6713845642035792\niteration:  3200 loss:  0.6504163858078991\niteration:  3220 loss:  0.6301408415714369\niteration:  3240 loss:  0.610533596484151\niteration:  3260 loss:  0.5915712277500971\niteration:  3280 loss:  0.5732311870791229\niteration:  3300 loss:  0.5554917649890482\niteration:  3320 loss:  0.5383320569706131\niteration:  3340 loss:  0.5217319313731409\niteration:  3360 loss:  0.5056719988767325\niteration:  3380 loss:  0.4901335834259871\niteration:  3400 loss:  0.47509869451084796\niteration:  3420 loss:  0.4605500006908174\niteration:  3440 loss:  0.446470804269947\niteration:  3460 loss:  0.4328450170404969\niteration:  3480 loss:  0.41965713702330143\niteration:  3500 loss:  0.4068922261419349\niteration:  3520 loss:  0.39453588877616796\niteration:  3540 loss:  0.3825742511472989\niteration:  3560 loss:  0.37099394149409426\niteration:  3580 loss:  0.35978207100334647\niteration:  3600 loss:  0.34892621546312946\niteration:  3620 loss:  0.33841439761040826\niteration:  3640 loss:  0.32823507014713527\niteration:  3660 loss:  0.3183770994012508\niteration:  3680 loss:  0.30882974961037857\niteration:  3700 loss:  0.2995826678073256\niteration:  3720 loss:  0.2906258692872295\niteration:  3740 loss:  0.28194972363700677\niteration:  3760 loss:  0.27354494130805185\niteration:  3780 loss:  0.2654025607136533\niteration:  3800 loss:  0.2575139358328796\niteration:  3820 loss:  0.2498707243029684\niteration:  3840 loss:  0.24246487598252497\niteration:  3860 loss:  0.2352886219682192\niteration:  3880 loss:  0.2283344640478587\niteration:  3900 loss:  0.22159516457322667\niteration:  3920 loss:  0.21506373673628598\niteration:  3940 loss:  0.2087334352329142\niteration:  3960 loss:  0.20259774729860716\niteration:  3980 loss:  0.1966503841012102\niteration:  4000 loss:  0.19088527247603354\niteration:  4020 loss:  0.18529654698933135\niteration:  4040 loss:  0.17987854231653502\niteration:  4060 loss:  0.1746257859221198\niteration:  4080 loss:  0.16953299102855082\niteration:  4100 loss:  0.16459504986217177\niteration:  4120 loss:  0.15980702716438253\niteration:  4140 loss:  0.1551641539570225\niteration:  4160 loss:  0.15066182155119134\niteration:  4180 loss:  0.14629557578932748\niteration:  4200 loss:  0.14206111151074502\niteration:  4220 loss:  0.13795426723123422\niteration:  4240 loss:  0.13397102002776293\niteration:  4260 loss:  0.13010748061976424\niteration:  4280 loss:  0.12635988863874434\niteration:  4300 loss:  0.12272460807844751\niteration:  4320 loss:  0.11919812291807083\niteration:  4340 loss:  0.11577703291139577\niteration:  4360 loss:  0.1124580495349835\niteration:  4380 loss:  0.10923799208896662\niteration:  4400 loss:  0.1061137839441306\niteration:  4420 loss:  0.10308244892938064\niteration:  4440 loss:  0.10014110785387853\niteration:  4460 loss:  0.09728697515840512\niteration:  4480 loss:  0.09451735569074708\niteration:  4500 loss:  0.09182964160012788\niteration:  4520 loss:  0.08922130934591879\niteration:  4540 loss:  0.08668991681609785\niteration:  4560 loss:  0.08423310055107351\niteration:  4580 loss:  0.08184857306871118\niteration:  4600 loss:  0.07953412028659737\niteration:  4620 loss:  0.07728759903769185\niteration:  4640 loss:  0.07510693467572607\niteration:  4660 loss:  0.07299011876686007\niteration:  4680 loss:  0.07093520686421614\niteration:  4700 loss:  0.06894031636211864\niteration:  4720 loss:  0.06700362442691812\niteration:  4740 loss:  0.06512336600149823\niteration:  4760 loss:  0.06329783188060722\niteration:  4780 loss:  0.06152536685433045\niteration:  4800 loss:  0.05980436791708438\niteration:  4820 loss:  0.058133282539680566\niteration:  4840 loss:  0.05651060700202884\niteration:  4860 loss:  0.05493488478423984\niteration:  4880 loss:  0.053404705013889386\niteration:  4900 loss:  0.0519187009673724\niteration:  4920 loss:  0.050475548623307206\niteration:  4940 loss:  0.049073965266062725\niteration:  4960 loss:  0.0477127081375362\niteration:  4980 loss:  0.04639057313541696\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}