{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec89f470",
   "metadata": {
    "papermill": {
     "duration": 0.004629,
     "end_time": "2025-10-11T16:02:17.302436",
     "exception": false,
     "start_time": "2025-10-11T16:02:17.297807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10112aee",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:17.312240Z",
     "iopub.status.busy": "2025-10-11T16:02:17.311511Z",
     "iopub.status.idle": "2025-10-11T16:02:21.614246Z",
     "shell.execute_reply": "2025-10-11T16:02:21.613429Z"
    },
    "papermill": {
     "duration": 4.30888,
     "end_time": "2025-10-11T16:02:21.615812",
     "exception": false,
     "start_time": "2025-10-11T16:02:17.306932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef27f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:21.625175Z",
     "iopub.status.busy": "2025-10-11T16:02:21.624723Z",
     "iopub.status.idle": "2025-10-11T16:02:21.654076Z",
     "shell.execute_reply": "2025-10-11T16:02:21.653068Z"
    },
    "papermill": {
     "duration": 0.035712,
     "end_time": "2025-10-11T16:02:21.655909",
     "exception": false,
     "start_time": "2025-10-11T16:02:21.620197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initializes matrix W and vector b\n",
    "classifier = nn.Linear(5, 10) # 5 inputs 10 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36111204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:21.664749Z",
     "iopub.status.busy": "2025-10-11T16:02:21.664489Z",
     "iopub.status.idle": "2025-10-11T16:02:21.742948Z",
     "shell.execute_reply": "2025-10-11T16:02:21.742051Z"
    },
    "papermill": {
     "duration": 0.084344,
     "end_time": "2025-10-11T16:02:21.744349",
     "exception": false,
     "start_time": "2025-10-11T16:02:21.660005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1558,  1.2278, -0.4035, -0.5563,  0.8764, -0.7221, -0.6799,  0.6084,\n",
       "        -1.2323,  1.1716])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Linear(10,3) # 10 inputs 3 outputs\n",
    "loss = nn.MSELoss()\n",
    "## dummy input x\n",
    "input_vector = torch.randn(10)\n",
    "input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e6d09e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:21.753297Z",
     "iopub.status.busy": "2025-10-11T16:02:21.753004Z",
     "iopub.status.idle": "2025-10-11T16:02:21.780597Z",
     "shell.execute_reply": "2025-10-11T16:02:21.779392Z"
    },
    "papermill": {
     "duration": 0.033908,
     "end_time": "2025-10-11T16:02:21.782093",
     "exception": false,
     "start_time": "2025-10-11T16:02:21.748185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  tensor([0.1870, 0.1573, 0.1697], grad_fn=<ViewBackward0>)\n",
      "Output:  tensor(0.2497, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## class number 3, denoted as a vector with the class index to 1\n",
    "target = torch.tensor([0,0,1])\n",
    "## y in math\n",
    "# passing the input vector to the model object of the linear classifier\n",
    "pred = model(input_vector)\n",
    "output = loss(pred, target) # loss is also object for MSE loss\n",
    "print(\"Prediction: \" ,pred)\n",
    "print(\"Output: \" , output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283a0a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:21.791132Z",
     "iopub.status.busy": "2025-10-11T16:02:21.790874Z",
     "iopub.status.idle": "2025-10-11T16:02:27.062298Z",
     "shell.execute_reply": "2025-10-11T16:02:27.061150Z"
    },
    "papermill": {
     "duration": 5.277665,
     "end_time": "2025-10-11T16:02:27.063910",
     "exception": false,
     "start_time": "2025-10-11T16:02:21.786245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4938, grad_fn=<MseLossBackward0>)\n",
      "epoch 0, loss 0.4938317537307739\n",
      "tensor(0.3161, grad_fn=<MseLossBackward0>)\n",
      "epoch 1, loss 0.3160523474216461\n",
      "tensor(0.2023, grad_fn=<MseLossBackward0>)\n",
      "epoch 2, loss 0.20227348804473877\n",
      "tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
      "epoch 3, loss 0.12945501506328583\n",
      "tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "epoch 4, loss 0.08285122364759445\n",
      "tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "epoch 5, loss 0.053024761378765106\n",
      "tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "epoch 6, loss 0.033935852348804474\n",
      "tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "epoch 7, loss 0.021718956530094147\n",
      "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "epoch 8, loss 0.013900130987167358\n",
      "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "epoch 9, loss 0.008896088227629662\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "\n",
    "    model = nn.Linear(4,2) # 4 inputs 2 outputs\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # Converting inputs and labels to Variable\n",
    "        inputs = torch.Tensor([0.8,0.4,0.4,0.2])\n",
    "        labels = torch.Tensor([1,0])\n",
    "\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(loss)\n",
    "\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdb63c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.073204Z",
     "iopub.status.busy": "2025-10-11T16:02:27.072847Z",
     "iopub.status.idle": "2025-10-11T16:02:27.079096Z",
     "shell.execute_reply": "2025-10-11T16:02:27.078116Z"
    },
    "papermill": {
     "duration": 0.012482,
     "end_time": "2025-10-11T16:02:27.080572",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.068090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(3, 20), # 3 input features, 20 output features\n",
    "        nn.ReLU(), # activation,=\n",
    "        nn.Linear(20,2) # 2 output classes\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdfc847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.089981Z",
     "iopub.status.busy": "2025-10-11T16:02:27.089279Z",
     "iopub.status.idle": "2025-10-11T16:02:27.095435Z",
     "shell.execute_reply": "2025-10-11T16:02:27.094608Z"
    },
    "papermill": {
     "duration": 0.011964,
     "end_time": "2025-10-11T16:02:27.096584",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.084620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(4,5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4,2)\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432bae6",
   "metadata": {
    "papermill": {
     "duration": 0.003654,
     "end_time": "2025-10-11T16:02:27.104247",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.100593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Universal approximation theorem:\n",
    "\n",
    "According to the universal approximation theorem, given enough neurons and the correct set of weights, a multi-layer NN can approximate any function. Learning this function is increasingly hard, and we have no guarantee that our data are enough to do so.\n",
    "\n",
    "Admittedly, that doesn’t mean we should only use NNs.\n",
    "\n",
    "In fact, we will learn about other models and how we can make a NN more compact, wider, or deeper to learn very rich data representations.\n",
    "\n",
    "Why is that even useful?\n",
    "\n",
    "Because NNs hide another secret besides being very good function approximators. They are also very good feature extractors.\n",
    "\n",
    "## Deep neural networks as feature extractors\n",
    "\n",
    "Feature extraction can be seen as the transformation of the input data points from the input space to the feature space where classification is much easier.\n",
    "\n",
    "Here is an intuitive and oversimplified example:\n",
    "\n",
    "Imagine that each data point has 70 dimensions. Finding the correct 70-dimensional function to distinguish the data into two categories is very difficult and time consuming.\n",
    "\n",
    "Instead, we transform our input to a three-dimensional space where a classifier can approximate the decision boundary more easily. If we transform the 3D decision boundary back to the 70-dimensional space, we will see that it corresponds to a 70-dimensional decision boundary.\n",
    "\n",
    "The transformed space does not always need to be low-dimensional, but high-dimensional spaces do not guarantee better results either.\n",
    "\n",
    "Think of the 70-dim example: if one of these input dimensions refers to the label, it would be enough to have 100% accuracy.\n",
    "\n",
    "In any case, this is the main reason Deep Neural Networks (DNNs) exist: to transform the input data into a “better” space. Better because we can classify the data more easily after we transform them!\n",
    "\n",
    "In fact, in most real-life applications, only the last one or two layers of a neural network performs the actual classification. The rest account for feature extraction and learning representations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2af48",
   "metadata": {
    "papermill": {
     "duration": 0.003522,
     "end_time": "2025-10-11T16:02:27.111503",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.107981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build a Neural Network With Pytorch\n",
    "Implement a vanilla neural network from scratch using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a6edfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.120004Z",
     "iopub.status.busy": "2025-10-11T16:02:27.119757Z",
     "iopub.status.idle": "2025-10-11T16:02:27.124728Z",
     "shell.execute_reply": "2025-10-11T16:02:27.123814Z"
    },
    "papermill": {
     "duration": 0.010785,
     "end_time": "2025-10-11T16:02:27.126053",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.115268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.tensor([1,2,3,4,5]) # 1d array\n",
    "Y = torch.tensor([[1,2], [3,4]]) # 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c4f5ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.135117Z",
     "iopub.status.busy": "2025-10-11T16:02:27.134883Z",
     "iopub.status.idle": "2025-10-11T16:02:27.142198Z",
     "shell.execute_reply": "2025-10-11T16:02:27.141181Z"
    },
    "papermill": {
     "duration": 0.013562,
     "end_time": "2025-10-11T16:02:27.143496",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.129934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def neuron(input):\n",
    "    W = torch.tensor([0.5,0.5,0.5]) # 3 x 1\n",
    "    b = torch.tensor([0.5]) # 1 x 1\n",
    "    return torch.add(torch.matmul(W, input), b)\n",
    "\n",
    "neuron(torch.tensor([5.0, 5.0, 5.0])) # 1 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8451d8",
   "metadata": {
    "papermill": {
     "duration": 0.003734,
     "end_time": "2025-10-11T16:02:27.151378",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.147644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that the linear layer does not contain the activation function, so we have to explicitly declare them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e5c5ad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.160259Z",
     "iopub.status.busy": "2025-10-11T16:02:27.159997Z",
     "iopub.status.idle": "2025-10-11T16:02:27.165651Z",
     "shell.execute_reply": "2025-10-11T16:02:27.164842Z"
    },
    "papermill": {
     "duration": 0.011861,
     "end_time": "2025-10-11T16:02:27.167131",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.155270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(2, 3),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(3, 2),\n",
    "        nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58043b1f",
   "metadata": {
    "papermill": {
     "duration": 0.0037,
     "end_time": "2025-10-11T16:02:27.175055",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.171355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Program your own neural network\n",
    "\n",
    "Model class is inheriting (taking all the properties and methods) from PyTorch's base class, torch.nn.Module.\n",
    "\n",
    "nn.Module is the fundamental building block for all neural network modules (layers, loss functions, and entire models) in PyTorch. It automatically tracks trainable parameters, manages data movement (like .cuda()), and handles the forward/backward pass logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7042e4ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.183785Z",
     "iopub.status.busy": "2025-10-11T16:02:27.183491Z",
     "iopub.status.idle": "2025-10-11T16:02:27.194568Z",
     "shell.execute_reply": "2025-10-11T16:02:27.193751Z"
    },
    "papermill": {
     "duration": 0.017055,
     "end_time": "2025-10-11T16:02:27.195959",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.178904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Model(\n",
      "  (linear1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=2, bias=True)\n",
      ")\n",
      "w1:  tensor([[ 0.4794, -0.3365],\n",
      "        [ 0.4806,  0.1790],\n",
      "        [-0.5990, -0.2774]])\n",
      "X:  tensor([[-2.4476, -0.9314]])\n",
      "Y:  tensor([[0.3655, 0.4365]], grad_fn=<SigmoidBackward0>)\n",
      "--- All Model Parameters ---\n",
      "Layer: linear1.weight | Shape: torch.Size([3, 2])\n",
      "Values:\n",
      "[[ 0.47944325 -0.3364593 ]\n",
      " [ 0.4806242   0.17897794]\n",
      " [-0.59902    -0.27735037]]\n",
      "\n",
      "Layer: linear1.bias | Shape: torch.Size([3])\n",
      "Values:\n",
      "[ 0.46587247 -0.5453316   0.5878149 ]\n",
      "\n",
      "Layer: linear2.weight | Shape: torch.Size([2, 3])\n",
      "Values:\n",
      "[[-0.38760534 -0.129304   -0.11307722]\n",
      " [-0.5074848   0.08875504 -0.37310243]]\n",
      "\n",
      "Layer: linear2.bias | Shape: torch.Size([2])\n",
      "Values:\n",
      "[-0.27563104  0.2767518 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(2, 3)\n",
    "        self.linear2 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, X): # modifying the forward method of base class, linear layers will automatically us this\n",
    "        h = torch.sigmoid(self.linear1(X)) # passes input to first hidden layer, multiplies it by weights and does activation (sigmoid) function\n",
    "        o = torch.sigmoid(self.linear2(h)) # passes output of first hidden layer to second hidden layer, does activation\n",
    "        return o\n",
    "\n",
    "model = Model()\n",
    "print('model: ', model)\n",
    "print('w1: ', model.linear1.weight.data)\n",
    "X = torch.randn((1,2)) # 1 row 2 cols\n",
    "print('X: ', X)\n",
    "Y = model(X)\n",
    "print('Y: ', Y)\n",
    "\n",
    "print(\"--- All Model Parameters ---\")\n",
    "# .named_parameters() returns (name, parameter) tuples\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name or 'bias' in name:\n",
    "        print(f\"Layer: {name} | Shape: {param.shape}\")\n",
    "        print(f\"Values:\\n{param.data.numpy()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb910189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.205392Z",
     "iopub.status.busy": "2025-10-11T16:02:27.205087Z",
     "iopub.status.idle": "2025-10-11T16:02:27.219483Z",
     "shell.execute_reply": "2025-10-11T16:02:27.218406Z"
    },
    "papermill": {
     "duration": 0.021246,
     "end_time": "2025-10-11T16:02:27.221347",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.200101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2195, 0.0326]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seed = 172\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "def fnn(input):\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(10, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "    )\n",
    "\n",
    "    return model(input)\n",
    "\n",
    "input = torch.randn((1,10))\n",
    "print(fnn(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855fb94",
   "metadata": {
    "papermill": {
     "duration": 0.003757,
     "end_time": "2025-10-11T16:02:27.229291",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.225534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optimization\n",
    "Explore the different variations of the gradient descent algorithm.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "The equation and code presented above actually referred to batch gradient descent. In this variant, we calculate the gradient for the entire dataset on each training step before updating the weights.\n",
    "\n",
    "```\n",
    "for t in range(steps):\n",
    "  dw = gradient(loss, data, w)\n",
    "  w = w - learning_rate *dw\n",
    "```\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "Stochastic Gradient Descent (SGD) was introduced to address this exact issue. Instead of calculating the gradient over all training examples and updating the weights, SGD updates the weights for each training example \n",
    "\n",
    "```\n",
    "for t in range(steps):\n",
    "  for example in data:\n",
    "    dw = gradient(loss, example, w)\n",
    "    w = w - learning_rate *dw\n",
    "```\n",
    "\n",
    "As a result, SGD is much faster and more computationally efficient, but it has noise in the estimation of the gradient. Since it updates the weight frequently, it can lead to big oscillations and that makes the training process highly unstable.\n",
    "\n",
    "We continuously walk in a zig-zag fashion down the landscape, which keeps overshooting and missing our minimum. However, we can easily get away from local minimums for the same reason, and keep searching for a better one.\n",
    "\n",
    "### Mini-batch Stochastic Gradient Descent\n",
    "Mini-batch SGD sits right in the middle of the two previous ideas and combines the best of both worlds. It randomly selects n training examples — the so-called mini-batch — from the whole dataset and computes the gradients only from them. It essentially tries to approximate Batch Gradient Descent by sampling only a subset of the data.\n",
    "\n",
    "```\n",
    "for t in range(steps):\n",
    "  for mini_batch in get_batches(data, batch_size):\n",
    "    dw = gradient(loss, mini_batch, w)\n",
    "    w = w - learning_rate *dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f5911",
   "metadata": {
    "papermill": {
     "duration": 0.003863,
     "end_time": "2025-10-11T16:02:27.237075",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.233212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Popular Optimization Algorithms\n",
    "A saddle point is a point on the surface of the graph of a function where the slopes (derivatives) are all zero but which is not a local maximum of the function.\n",
    "\n",
    "### Adding momentum\n",
    "\n",
    "```\n",
    "for t in range(steps):\n",
    "    dw = gradient(loss, w)\n",
    "    v = rho*v +dw   # velocity decayed by friction\n",
    "    w = w - learning_rate *v\n",
    "```\n",
    "\n",
    "### Adaptive learning rate\n",
    "perform smaller updates for frequent features and bigger ones for infrequent ones.\n",
    "\n",
    "#### Adagrad\n",
    "Adagrad keeps a running sum of the squares of the gradients in each dimension, and in each update, we scale the learning rate based on the sum. That way we achieve a different learning rate for each parameter (or an adaptive learning rate). Moreover, by using the root of the squared gradients, we only take into account the magnitude of the gradients and not the sign.\n",
    "\n",
    "```\n",
    "for t in range(steps):\n",
    "    dw = gradient(loss, w)\n",
    "    squared_gradients +=dw*dw\n",
    "    w = w - learning_rate * dw/ (squared_gradients.sqrt() + e)\n",
    "```\n",
    "\n",
    "A big drawback of Adagrad is that as time goes by, the learning rate becomes smaller and smaller due to the monotonic increment of the running squared sum.\n",
    "\n",
    "#### RMSprop\n",
    "A solution to this problem is a modification of the above algorithm called RMSProp, which can be thought of as a “Leaky Adagrad”. In essence, we once again add the notion of friction by decaying the sum of the previous squared gradients.\n",
    "\n",
    "As we did in momentum-based methods, we multiply our term (here the running squared sum) with a constant value (the decay rate). That way we hope that the algorithm will not slow down over the course of training as Adagrad does.\n",
    "\n",
    "```\n",
    "for t in range(steps):\n",
    "    dw = gradient(loss, w)\n",
    "    squared_gradients = decay_rate*squared_gradients + (1- decay_rate)* dw*dw\n",
    "    w = w - learning_rate * (dw/(squared_gradients.sqrt() + e)\n",
    "```\n",
    "\n",
    "#### Adam - used most commonly for DL\n",
    "Adam (Adaptive moment estimation) is arguably the most popular variation nowadays. It has been used extensively in both research and business applications. Its popularity is hidden in the fact that it combines the two best previous ideas, momentum and adaptive learning rate.\n",
    "\n",
    "We now keep track of two running variables, velocity and the squared gradients average we described in RMSProp. They are also called first and second moments in the original paper.\n",
    "\n",
    "```\n",
    "for t in range(steps):\n",
    "    dw = gradient(loss, w)\n",
    "    moment1= delta1 *moment1  +(1-delta1)* dw\n",
    "    moment2 = delta2*moment2 +(1-delta2)*dw*dw\n",
    "    moment1_unbiased = moment1  /(1-delta1**t)\n",
    "    moment2_unbiased = moment2  /(1-delta2**t)\n",
    "    w = w - learning_rate*moment1_unbiased/ (moment2_unbiased.sqrt()+e)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25524a1a",
   "metadata": {
    "papermill": {
     "duration": 0.003628,
     "end_time": "2025-10-11T16:02:27.244709",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.241081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Activation Functions\n",
    "Activation functions are applied element-wise and, as a result, are independent of the input shape.\n",
    "\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- ReLU\n",
    "- Leaky Relu\n",
    "- Parametric Relu\n",
    "- Softmax - We usually apply softmax in the last dimension of a multi-dimensional input. To do that in Pytorch, you can just set dim=-1.\n",
    "     ```\n",
    "  nn.Softmax(dim=-1)\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a11e87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.254466Z",
     "iopub.status.busy": "2025-10-11T16:02:27.253329Z",
     "iopub.status.idle": "2025-10-11T16:02:27.258946Z",
     "shell.execute_reply": "2025-10-11T16:02:27.258102Z"
    },
    "papermill": {
     "duration": 0.011826,
     "end_time": "2025-10-11T16:02:27.260400",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.248574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def m_sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "def m_tanh(x):\n",
    "    return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
    "\n",
    "def m_relu(x):\n",
    "    return torch.mul(x, (x > 0))\n",
    "\n",
    "\n",
    "def m_softmax(x):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ca72b",
   "metadata": {
    "papermill": {
     "duration": 0.004096,
     "end_time": "2025-10-11T16:02:27.268710",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.264614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training in Pytorch\n",
    "We will build a network with an input of size 3072, 3 linear layers with dimensions 128, 64, 10, and 2 Relu layers in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c94d348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T16:02:27.277925Z",
     "iopub.status.busy": "2025-10-11T16:02:27.277590Z",
     "iopub.status.idle": "2025-10-11T16:02:27.282541Z",
     "shell.execute_reply": "2025-10-11T16:02:27.281798Z"
    },
    "papermill": {
     "duration": 0.011193,
     "end_time": "2025-10-11T16:02:27.283865",
     "exception": false,
     "start_time": "2025-10-11T16:02:27.272672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## 1. DEFINE MODEL HERE\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(3072, 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(128, 64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(64, 10)\n",
    "# )\n",
    "\n",
    "# def train():\n",
    "\n",
    "#     training_data = load_data()\n",
    "    \n",
    "#     # 2. LOSS AND OPTIMIZER\n",
    "#     criteria = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay=1e-4)\n",
    "    \n",
    "    \n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for epoch in range(10):\n",
    "#         for i, data in enumerate(training_data, 0):\n",
    "           \n",
    "#             inputs, labels = data\n",
    "#             #reshape images so they can be fed to a nn.Linear()\n",
    "#             inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "#             optimizer.zero_grad() # reset gradients\n",
    "            \n",
    "#             ##3. RUN BACKPROP\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criteria(outputs, labels)\n",
    "#             loss.backward() # compute gradients\n",
    "#             optimizer.step() # update model parameters\n",
    "            \n",
    "           \n",
    "           \n",
    "\n",
    "#             # print statistics\n",
    "#             running_loss += loss.item()\n",
    "#             if i % 500 == 499:    # print every 500 mini-batches\n",
    "#                 print('[%d, %5d] loss: %.3f' %\n",
    "#                       (epoch + 1, i + 1, running_loss / 500))\n",
    "#                 running_loss = 0.0\n",
    "                \n",
    "#     print('Training finished')\n",
    "    \n",
    "\n",
    "# train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.694732,
   "end_time": "2025-10-11T16:02:28.809758",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-11T16:02:13.115026",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
